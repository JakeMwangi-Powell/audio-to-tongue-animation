# Speech Audio to tongue animation

This MSc project focuses on advancing the realism of digital character animations by developing a method to synthesize tongue movements that accurately align with voice inputs, ensuring synchronized and lifelike speech animation. Utilizing a dataset comprising of dialogue and tongue position movements, the project encompasses a comprehensive workflow that includes an in-depth literature review, environment setup, data pre-processing, the development of a synthesis methodology, and system evaluation. The literature review identifies gaps and opportunities in current character animation techniques, particularly in the relatively underexplored area of tongue movement synthesis.
The core of the project lies in the development of a synthesis approach that leverages machine learning models to generate realistic tongue movements dynamically based on voice inputs. Through rigorous experimentation, various model architectures, including RNNs, LSTMs, and transformers, were evaluated. The GRU model was identified as the most effective with optimal performance attributed to its simplicity, efficiency, and bidirectionality. Hyperparameter tuning further refined the model, revealing that configurations such as an initial learning rate of 0.001, a hidden dimension of 256, and enabling bidirectionality, were crucial for capturing sequential dependencies and preventing overfitting. The optimisation of this model through its hyperparameters yielded a final MSE loss of 1.473.
Final evaluations demonstrated the selected configurations for the model resulted in high-quality automated tongue animations. This project contributes to the field of character animation by offering a novel approach that enhances the lifelikeness of digital speech, with potential applications in games, films, and virtual reality environments. The findings demonstrate the applicability of machine learning methodologies in achieving realistic and synchronized tongue movements in animated characters.
